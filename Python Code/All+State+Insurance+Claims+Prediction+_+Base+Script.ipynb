{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W5XWdNvtGG3"
   },
   "source": [
    "# All State Insurance Claim Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdGBVJ2_tGG8"
   },
   "source": [
    "### The notebook is intended to cover the following concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sreuBH3_tGG8"
   },
   "source": [
    "<ul>\n",
    "<li>Business problem</li>\n",
    "<li>Dataset overview</li>\n",
    "<li>Exploratory data analysis</li>\n",
    "<li>Data cleaning and Pre-Processing</li>\n",
    "<li>Outlier treatment</li>\n",
    "<li>Feature selection techniques</li>\n",
    "<li>Machine learning models</li>\n",
    "<li>Hyperparameter tuning</li>\n",
    "<li>Model validation</li>\n",
    "<li>Deployment using Flask API</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjxZo_NNtGG9"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGVxOcH4tGG-"
   },
   "source": [
    "##### Import the packages and load the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to TrueFoundry  ðŸŽ‰\n",
    "\n",
    "1. An account with  <a href=\"https://projectpro.truefoundry.com/signin\">TrueFoundry</a>. has been created with the same email address that you use to sign in to ProjectPro and an email has been sent to you to set your password. \n",
    "2. Please go to your inbox and follow the link to make sure you are logged into TrueFoundry before getting to the next cell. If you don't see the email in your inbox, please check your Spam folder. \n",
    "\n",
    "Note: If you are not able to signin or did not receive an email, please send an email to nikunj@truefoundry.com with the following subject- \"ProjectPro User: TrueFoundry Login Issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6-GaGUtzJML"
   },
   "outputs": [],
   "source": [
    "# !pip install mlfoundry --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_epfY4_htGG-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import chi2\n",
    "import math\n",
    "import pickle\n",
    "import shap\n",
    "\n",
    "#modify the display options to view entire dataframe\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv',\n",
       " '__MACOSX/',\n",
       " '__MACOSX/._test.csv',\n",
       " 'train.csv',\n",
       " '__MACOSX/._train.csv',\n",
       " 'test_data_subset.csv',\n",
       " '__MACOSX/._test_data_subset.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "folder = urllib.request.urlopen('https://s3.amazonaws.com/hackerday.datascience/50/dataset.zip')\n",
    "zipfile = ZipFile(BytesIO(folder.read()))\n",
    "zipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dqlCLYy3tGHA"
   },
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv(zipfile.open(\"train.csv\"))|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2G3vl-30Jxa"
   },
   "outputs": [],
   "source": [
    "import mlfoundry as mlf\n",
    "\n",
    "TRACKING_URL = 'https://projectpro.truefoundry.com'\n",
    "mlf_api = mlf.get_client(TRACKING_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqrb8bqKtGHA"
   },
   "source": [
    "##### Do the following:\n",
    "<ul>\n",
    "<li>Analyze the size of training data</li>\n",
    "<li>Verify the first few observations</li>\n",
    "<li>Check the column headers</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du9HibrmtGHB"
   },
   "outputs": [],
   "source": [
    "# check for the shape of train data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4IPPrYotGHB"
   },
   "outputs": [],
   "source": [
    "# check for first 5 rows\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ci20vFXtGHC"
   },
   "outputs": [],
   "source": [
    "# check for all the columns present\n",
    "column_names = np.array(train_data.columns)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJbGtWkTtGHC"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-uLOiqotGHC"
   },
   "source": [
    "###### Identify the categorical and numerical columns to check the data distribution and 5 point summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wz8q-KUmtGHD"
   },
   "outputs": [],
   "source": [
    "column_datatypes = train_data.dtypes\n",
    "categorical_columns = list(column_datatypes[column_datatypes==\"object\"].index.values)\n",
    "continuous_columns = list(column_datatypes[column_datatypes==\"float64\"].index.values)\n",
    "continuous_columns.remove('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlxVkHuFtGHD"
   },
   "source": [
    "##### check the distribution of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiDjkV-_tGHD"
   },
   "outputs": [],
   "source": [
    "#function to check the distribution of values in categorical columns\n",
    "#Training data and Categorical columns list\n",
    "def category_distribution(train_data,categorical_columns):\n",
    "    categorical_column_distribution = list()\n",
    "    for cat_column in categorical_columns:\n",
    "        categorical_column_distribution.append(train_data[cat_column].value_counts())\n",
    "    return(categorical_column_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O47usqgZtGHD"
   },
   "outputs": [],
   "source": [
    "# check for categorical column distribution\n",
    "categorical_column_distribution = category_distribution(train_data,categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOpJ0navtGHE"
   },
   "outputs": [],
   "source": [
    "categorical_column_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1FfDewMtGHE"
   },
   "outputs": [],
   "source": [
    "length_categorical_columns = list(map(lambda x:len(x),categorical_column_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMTCclzZtGHE"
   },
   "outputs": [],
   "source": [
    "#count the number of columns having the same number of unique values\n",
    "distribution_dict = dict()\n",
    "for val in length_categorical_columns:\n",
    "    if val in distribution_dict.keys():\n",
    "        count = distribution_dict[val]\n",
    "        distribution_dict[val] = count+1\n",
    "    else:\n",
    "        distribution_dict[val]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A27jEfagtGHE"
   },
   "outputs": [],
   "source": [
    "distribution_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAa4uT-NtGHF"
   },
   "source": [
    "### Plot a bar-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2_VjGEGtGHF"
   },
   "outputs": [],
   "source": [
    "#plot showing the count of columns having same number of unique values\n",
    "keys = distribution_dict.keys()\n",
    "values = distribution_dict.values()\n",
    "plt.bar(keys, values,width=0.8)\n",
    "plt.xlabel('Distinct Values in Categorical Variable', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "plt.title('Categorical Labels with Same Unique Values',fontsize=20)\n",
    "plt.rcParams['figure.figsize'] = [48/2.54, 10/2.54]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBcjV9brtGHF"
   },
   "source": [
    "##### check the distribution of continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gY3r27QttGHG"
   },
   "outputs": [],
   "source": [
    "#filter out the continous columns and view the descriptive statistics\n",
    "train_data[continuous_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7KJIP7ytGHG"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jATdV0extGHG"
   },
   "source": [
    "#### Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50pcinsc0fDS"
   },
   "outputs": [],
   "source": [
    "# creating mlfoundry run to log dataset related plots\n",
    "mlf_run = mlf_api.create_run(\"insurance\", \"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzm94Mf1tGHH"
   },
   "outputs": [],
   "source": [
    "#Check if there is any missing value in the columuns\n",
    "#value of 0 indicates no missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "np.max(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X2Pyh6otGHH"
   },
   "outputs": [],
   "source": [
    "#Manually insert a blank value across 5 rows\n",
    "total_rows = train_data.shape[0]\n",
    "columns_with_blanks_cat = np.random.randint(1,116,2)\n",
    "columns_with_blanks_cont = np.random.randint(117,130,3)\n",
    "columns_with_blank = np.append(columns_with_blanks_cat,columns_with_blanks_cont)\n",
    "\n",
    "#for every column insert 5 blanks at random locations\n",
    "for col in columns_with_blank:\n",
    "    rows_with_blanks = np.random.randint(1,total_rows,5)\n",
    "    train_data.iloc[rows_with_blanks,col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fliu6oTRtGHH"
   },
   "outputs": [],
   "source": [
    "#Validate the number of columns with missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "np.max(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnO2kicQtGHI"
   },
   "outputs": [],
   "source": [
    "#Displaying the columns with missing values\n",
    "columns_with_missing = train_data.columns[train_data.isnull().any()]\n",
    "print(columns_with_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiRtH7PZtGHI"
   },
   "source": [
    "##### Data Preprocessing class with the following functions:\n",
    "<ul>\n",
    "    <li><b>missing_value_continuous</b>: function to handle missing values of continuous variables</li>\n",
    "    <li><b>missing_value_categorical</b>: function to handle missing values of categorical variables</li>\n",
    "    <li><b>outlier_treatment</b>: function to handle continuous outliers in the dataset</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FvHBq1ztGHI"
   },
   "outputs": [],
   "source": [
    "class Data_preprocessing:\n",
    "    def __init__(self,train_data):\n",
    "        self.train_data = train_data\n",
    "    \n",
    "    def missing_value_continuous(self,column_names_with_specific_type,imputation_type=\"mean\"): # null value imputation with mean value\n",
    "        if imputation_type==\"mean\": # mean imputation \n",
    "            mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            mean_imputer.fit(self.train_data[column_names_with_specific_type])\n",
    "            self.train_data[column_names_with_specific_type]=mean_imputer.transform(self.train_data[column_names_with_specific_type])\n",
    "        if imputation_type==\"median\": # median imputation\n",
    "            median_imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "            median_imputer.fit(self.train_data[column_names_with_specific_type])\n",
    "            self.train_data[column_names_with_specific_type]=median_imputer.transform(self.train_data[column_names_with_specific_type])\n",
    "        return self.train_data\n",
    "    \n",
    "    def missing_value_categorical(self,column_names_with_specific_type,imputation_type=\"most_frequent\"): # check for missing categorical column values\n",
    "        most_frequent = SimpleImputer(strategy=\"most_frequent\")\n",
    "        most_frequent.fit(self.train_data[column_names_with_specific_type])\n",
    "        self.train_data[column_names_with_specific_type] = most_frequent.transform(train_data[column_names_with_specific_type])\n",
    "        return self.train_data\n",
    "    \n",
    "    def outlier_treatment(self,Q1,Q3,IQR,columns_with_outlier,action): # outlier treatmenr\n",
    "        if action==\"median\":\n",
    "            for i in range(len(columns_with_outlier)):\n",
    "                column_name = columns_with_outlier[i]\n",
    "                meadian_outlier = np.median(self.train_data[column_name])\n",
    "                self.train_data.loc[self.train_data[((self.train_data[column_name]<(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]>(Q3[column_name]+(1.5*IQR[column_name]))))].index,column_name]=meadian_outlier\n",
    "        if action==\"mean\":\n",
    "            for i in range(len(columns_with_outlier)):\n",
    "                column_name = columns_with_outlier[i]\n",
    "                mean_outlier = np.mean(self.train_data[column_name])\n",
    "                self.train_data.loc[self.train_data[((self.train_data[column_name]<(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]>(Q3[column_name]+(1.5*IQR[column_name]))))].index,column_name]=mean_outlier\n",
    "        if action==\"remove\":\n",
    "            for i in range(len(columns_with_outlier)):\n",
    "                column_name = columns_with_outlier[i]\n",
    "                self.train_data = self.train_data[~((self.train_data[column_name]<(Q1[column_name]-(1.5*IQR[column_name])))|(self.train_data[column_name]>(Q3[column_name]+(1.5*IQR[column_name]))))]\n",
    "        return self.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xITwb2NstGHI"
   },
   "outputs": [],
   "source": [
    "#apply data preprocessing\n",
    "Data_preprocessing_obj = Data_preprocessing(train_data)\n",
    "train_data = Data_preprocessing_obj.missing_value_continuous(continuous_columns,\"median\")\n",
    "train_data = Data_preprocessing_obj.missing_value_categorical(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "La6V4ZEwtGHI"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTmHEeeQtGHI"
   },
   "source": [
    "##### Section on handling outliers in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMh0fwumtGHI"
   },
   "outputs": [],
   "source": [
    "# plot boxplots to check the outliers\n",
    "ax = sns.boxplot(data=train_data[continuous_columns], orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5XsbhPltGHJ"
   },
   "outputs": [],
   "source": [
    "# our columns with outliers\n",
    "columns_with_outlier = ['cont7','cont9','cont10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkx4F-MgtGHJ"
   },
   "outputs": [],
   "source": [
    "#compute the interquartile range for all continuous columns\n",
    "Q1 = train_data[continuous_columns].quantile(0.25)\n",
    "Q3 = train_data[continuous_columns].quantile(0.75)\n",
    "IQR = (Q3-Q1)\n",
    "train_data = Data_preprocessing_obj.outlier_treatment(Q1,Q3,IQR,columns_with_outlier,\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50bMU4ACtGHJ"
   },
   "outputs": [],
   "source": [
    "# plot boxplot \n",
    "ax = sns.boxplot(data=train_data[continuous_columns], orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhk_nq9gtGHJ"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUErASNstGHJ"
   },
   "source": [
    "##### Feature elimination techniques for continuous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kAOLN5RtGHJ"
   },
   "outputs": [],
   "source": [
    "#Function for feature selection of numeric variables\n",
    "#Remove variables with constant variance\n",
    "#Remove variables with Quasi-Constant variance with a fixed threshold\n",
    "#Remove correlated variables\n",
    "\n",
    "def feature_selection_numerical_variables(train_data,qthreshold,corr_threshold,exclude_numerical_cols_list):\n",
    "    num_colums = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numerical_columns = list(train_data.select_dtypes(include=num_colums).columns)\n",
    "    numerical_columns = [column for column in numerical_columns if column not in exclude_numerical_cols_list]\n",
    "    \n",
    "    #remove variables with constant variance\n",
    "    constant_filter = VarianceThreshold(threshold=0)\n",
    "    constant_filter.fit(train_data[numerical_columns])\n",
    "    constant_columns = [column for column in train_data[numerical_columns].columns \n",
    "                    if column not in train_data[numerical_columns].columns[constant_filter.get_support()]]\n",
    "    if len(constant_columns)>0:\n",
    "        train_data.drop(labels=constant_columns, axis=1, inplace=True)\n",
    "\n",
    "    #remove deleted columns from dataframe\n",
    "    numerical_columns = [column for column in numerical_columns if column not in constant_columns]\n",
    "        \n",
    "    #remove variables with qconstant variance\n",
    "    #Remove quasi-constant variables\n",
    "    qconstant_filter = VarianceThreshold(threshold=qthreshold)\n",
    "    qconstant_filter.fit(train_data[numerical_columns])\n",
    "    qconstant_columns = [column for column in train_data[numerical_columns].columns \n",
    "                         if column not in train_data[numerical_columns].columns[constant_filter.get_support()]]\n",
    "    if len(qconstant_columns)>0:\n",
    "        train_data.drop(labels=qconstant_columns, axis=1, inplace=True)\n",
    "    \n",
    "    #remove deleted columns from dataframe\n",
    "    numerical_columns = [column for column in numerical_columns if column not in qconstant_columns]\n",
    "    \n",
    "    #remove correlated variables\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = train_data[numerical_columns].corr()\n",
    "    ax = sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True)\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=45,\n",
    "        horizontalalignment='right');\n",
    "    #print(correlation_matrix)\n",
    "\n",
    "    mlf_run.log_plots({\"correlation-matrix\": plt})\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > corr_threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                colcompared = correlation_matrix.columns[j]\n",
    "                #check if the column compared against is not in the columns excluded list\n",
    "                if colcompared not in correlated_features:\n",
    "                    correlated_features.add(colname)\n",
    "    train_data.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "    \n",
    "    return train_data,constant_columns,qconstant_columns,correlated_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvyp0FSFtGHJ"
   },
   "outputs": [],
   "source": [
    "train_data,constant_columns,qconstant_columns,correlated_features =feature_selection_numerical_variables(train_data,0.01,0.75,['loss','id'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7Ed2TOOtGHK"
   },
   "outputs": [],
   "source": [
    "correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6I0b9qdtGHK"
   },
   "source": [
    "##### Handling correlation between categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUO-aCUOtGHK"
   },
   "outputs": [],
   "source": [
    "# save the encoders to disk to be fitted on test data\n",
    "for cf1 in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data[cf1].unique())\n",
    "    filename = cf1+\".sav\"\n",
    "    pickle.dump(le, open(filename, 'wb'))\n",
    "    train_data[cf1] = le.transform(train_data[cf1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XLakNaTtGHK"
   },
   "outputs": [],
   "source": [
    "#snippet to calculate the unique values with a categorical columns\n",
    "df = pd.DataFrame(columns=[\"Column_Name\",\"Count\"])\n",
    "for cat in categorical_columns:\n",
    "    unique_value_count = len(train_data[cat].unique())\n",
    "    df = df.append({'Column_Name': cat, \"Count\":int(unique_value_count)}, ignore_index=True)\n",
    "columns_unique_value = np.array(df.Count.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R5me45utGHK"
   },
   "outputs": [],
   "source": [
    "#snippet to identify the dependent/correlated categorical variables and drop them\n",
    "columns_to_drop_cat = set()\n",
    "correlated_columns = dict()\n",
    "for unique_value_count in columns_unique_value:\n",
    "    if unique_value_count>1:\n",
    "        categorical_columns = df.loc[df.Count==unique_value_count,'Column_Name']\n",
    "        categorical_columns = categorical_columns.reset_index(drop=True)\n",
    "        columns_length=len(categorical_columns)\n",
    "        for col in range(columns_length-1):\n",
    "            column_to_compare = categorical_columns[col]\n",
    "            columns_compare_against = categorical_columns[(col+1):columns_length]\n",
    "            chi_scores = chi2(train_data[columns_compare_against],train_data[column_to_compare])\n",
    "            if column_to_compare not in columns_to_drop_cat:\n",
    "                columns_to_be_dropped = [i for i in range(len(columns_compare_against)) if chi_scores[1][i]<=0.05]\n",
    "                columns_to_drop_array = np.array(columns_compare_against)[columns_to_be_dropped]\n",
    "                correlated_columns[column_to_compare]=columns_to_drop_array\n",
    "                columns_to_drop_cat.update(columns_to_drop_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0XX-vJwtGHK"
   },
   "outputs": [],
   "source": [
    "# drop columns that are not needed\n",
    "train_data = train_data.drop(columns_to_drop_cat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHqbcEbntGHK"
   },
   "outputs": [],
   "source": [
    "correlated_features = list(correlated_features)\n",
    "columns_to_drop_cat = list(columns_to_drop_cat)\n",
    "columns_to_drop_cat.extend(correlated_features)\n",
    "columns_to_drop = columns_to_drop_cat.copy()\n",
    "\n",
    "#output the columns_to_drop file to a csv\n",
    "columns_to_drop_df=pd.DataFrame(columns_to_drop,columns=['colnames'])\n",
    "# columns_to_drop_df.to_csv(\"./model/columns_to_drop.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N9Tr5GWtGHK"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBw0XAWXtGHL"
   },
   "source": [
    "##### Visualizing the Output Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ne7OlqbitGHL"
   },
   "outputs": [],
   "source": [
    "#Visualizing the distribution of loss value\n",
    "# Density Plot and Histogram of loss\n",
    "sns.distplot(train_data['loss'], hist=True, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n",
    "\n",
    "mlf_run.log_plots({\"loss-histogram\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qFKVNPltGHL"
   },
   "outputs": [],
   "source": [
    "#We will use a log transformation on the dependent variable to reduce the scale\n",
    "train_data['loss'] = np.log(train_data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quH3VYmitGHL"
   },
   "outputs": [],
   "source": [
    "# Visualizing the distribution of loss value\n",
    "# Density Plot and Histogram of loss\n",
    "sns.distplot(train_data['loss'], hist=True, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQDtTh5OtGHL"
   },
   "outputs": [],
   "source": [
    "#taking a anti-log to transform the variable back to its original scale\n",
    "sns.distplot(np.exp(train_data['loss']), hist=True, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4BkQ2HT12Nf"
   },
   "outputs": [],
   "source": [
    "# ending the run after it is used\n",
    "mlf_run.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ZFUP_TtGHL"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RefUsG8ztGHL"
   },
   "source": [
    "##### Fit an ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owCM6pLvtGHL"
   },
   "outputs": [],
   "source": [
    "# import necessary models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clmjrQp_tGHL"
   },
   "outputs": [],
   "source": [
    "#convert the int64 columns categorical\n",
    "Column_datatypes= train_data.dtypes\n",
    "Integer_columns = list(Column_datatypes.where(lambda x: x ==\"int64\").dropna().index.values)\n",
    "train_data[Integer_columns] = train_data[Integer_columns].astype('category',copy=False)\n",
    "X,y = train_data.drop(['id','loss'],axis=1),train_data['loss']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFGgjC5itGHM"
   },
   "outputs": [],
   "source": [
    "# Instantiate model with 100 decision trees\n",
    "rf_base = RandomForestRegressor(n_estimators = 100, random_state = 42,oob_score = True)\n",
    "rf_base.fit(X_train, y_train) # fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfF8Oju0tGHM"
   },
   "outputs": [],
   "source": [
    "#save the model output\n",
    "pickle.dump(rf_base, open(\"basemodel_rf\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NI4STOK2tGHM"
   },
   "outputs": [],
   "source": [
    "#load the saved model and predict on the test data\n",
    "basedmodel_rf = pickle.load(open(\"basemodel_rf\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ubDKUYLy0q7"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_model(rf_base, mlf.ModelFramework.SKLEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCYF_GfVtGHM"
   },
   "outputs": [],
   "source": [
    "#validate the accuracy of the base model\n",
    "#compare the model accuracies\n",
    "Y_test_predict_base = rf_base.predict(X_test)\n",
    "print(\"Base model accuracy:\",np.sqrt(mean_squared_error(y_test, Y_test_predict_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lARK5oBFy0q7"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_metrics({\"mse\": np.sqrt(mean_squared_error(y_test, Y_test_predict_base))})\n",
    "mlf_run.log_params(rf_base.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuA6gvs5y0q7"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_dataset(\n",
    "    dataset_name = 'test',\n",
    "    features = X_test,\n",
    "    actuals = y_test,\n",
    "    predictions = rf_base.predict(X_test)\n",
    ") \n",
    "mlf_run.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyMOSkdetGHM"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0K_0CVAtGHM"
   },
   "source": [
    "###### HyperParameter Tuning Using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By6NaDs_tGHM"
   },
   "outputs": [],
   "source": [
    "#number of trees\n",
    "n_estimators = [100,200,300,400,500]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [200,400,600]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQIrcyCktGHM"
   },
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# 5 fold cross validation, \n",
    "# search across 150 different combinations, and use all available cores\n",
    "rf_tuned = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, cv = 3,n_iter = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "#save the model output\n",
    "pickle.dump(rf_tuned, open(\"tunedmodel_rf\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fPUFM3Gy0q8"
   },
   "outputs": [],
   "source": [
    "mlf_run = mlf_api.create_run(\"insurance-claim-prediction\", \"grid-search-model\")\n",
    "mlf_run.log_model(rf_tuned, mlf.ModelFramework.SKLEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhfM2cKStGHM"
   },
   "outputs": [],
   "source": [
    "#check the best params\n",
    "mlf_run.log_params(rf_tuned.best_params_)\n",
    "rf_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JP0a5dT8y0q9"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_metrics({\"mse\": np.sqrt(mean_squared_error(y_test, rf_tuned.predict(X_test)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZ47ahgptGHM"
   },
   "outputs": [],
   "source": [
    "#load the saved model and predict on the test data\n",
    "tunedmodel_rf = pickle.load(open(\"tunedmodel_rf\", 'rb'))\n",
    "\n",
    "Y_test_predict_tuned = rf_tuned.predict(X_test)\n",
    "print(\"Tuned model accuracy:\",np.sqrt(mean_squared_error(y_test, Y_test_predict_tuned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nr-yNogy0q9"
   },
   "outputs": [],
   "source": [
    "mlf_run.log_dataset(\n",
    "    dataset_name = 'test',\n",
    "    features = X_test,\n",
    "    actuals = y_test,\n",
    "    predictions = rf_tuned.predict(X_test)\n",
    ")\n",
    "mlf_run.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S_TQ0Z4tGHN"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq1-gINjtGHN"
   },
   "source": [
    "##### fit a GBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMlreM0WtGHN"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "gbm_base = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=3,\n",
    "    learning_rate=1.0)\n",
    "\n",
    "gbm_base.fit(X_train,y_train) # fit the gbm moodel\n",
    "\n",
    "#save the GBM model\n",
    "pickle.dump(gbm_base, open(\"basemodel_GBM\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRCv2RFbtGHN"
   },
   "outputs": [],
   "source": [
    "#load the saved model and predict on the test data\n",
    "basemodel_GBM = pickle.load(open(\"basemodel_GBM\", 'rb'))\n",
    "\n",
    "Y_test_predict_tuned = basemodel_GBM.predict(X_test)\n",
    "print(\"Base model GBM accuracy:\",np.sqrt(mean_squared_error(y_test, Y_test_predict_tuned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gYI3KwdtGHN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "All+State+Insurance+Claims+Prediction+_+Base+Script.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "24b66435a475fffbb2f9b7c14a6d1fa9afcbaee4bece33cc57192f830ef2bc81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
